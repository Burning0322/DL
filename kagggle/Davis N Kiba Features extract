{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10959248,"sourceType":"datasetVersion","datasetId":6817938}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport os\nimport re\nfrom transformers import T5Tokenizer, T5EncoderModel\nfrom torch.nn.parallel import DataParallel\n\n# 设置环境变量以减少内存碎片\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# 读取和处理数据\nwith open('/kaggle/input/davis-n-kiba/Davis.txt', 'r') as f:\n    lines = f.readlines()\n\ndata = []\nfor line in lines:\n    parts = line.strip().split(' ', 4)\n    if len(parts) == 5:\n        compound_id, protein_name, smiles, rest = parts[0], parts[1], parts[2], parts[3] + ' ' + parts[4]\n        sequence, label = rest.rsplit(' ', 1)\n        data.append({\n            'compound_id': compound_id,\n            'protein_name': protein_name,\n            'smiles': smiles,\n            'sequence': sequence,\n            'label': int(label)\n        })\n\n# 提取唯一蛋白序列\nproteins = set()\nfor protein in data:\n    proteins.add(protein['sequence'])\nproteins = list(proteins)\nprint(f\"Number of unique proteins: {len(proteins)}\")\n\n# 预处理序列\nsequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in proteins]\n\n# 设置设备\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 加载 tokenizer 和模型\nfrom transformers import T5Tokenizer, T5EncoderModel\n\n# local_path_model = \"autodl-tmp/prot_t5_xl_uniref50\"\nlocal_path_model = \"Rostlab/prot_t5_xl_uniref50\"\ntokenizer = T5Tokenizer.from_pretrained(local_path_model, legacy=False)  # 设置 legacy=False 使用新行为\n\n# 使用半精度加载模型\nmodel = T5EncoderModel.from_pretrained(local_path_model, torch_dtype=torch.float16)\n#model = T5EncoderModel.from_pretrained(local_path_model)\n\n# 如果有多个 GPU，使用 DataParallel\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = DataParallel(model)\n\nmodel = model.to(device)\nmodel.eval()\n\n# 分批处理序列\nbatch_size = 32\nembeddings = []\nmax_length = 1000\n\nfor i in range(0, len(sequence_examples), batch_size):\n    batch_sequences = sequence_examples[i:i + batch_size]\n    \n    # 明确指定 padding='max_length' 以确保所有序列填充到 max_length\n    inputs = tokenizer(batch_sequences, return_tensors=\"pt\", padding=\"max_length\", \n                       truncation=True, max_length=max_length).to(device)\n    \n    # 前向传播，无梯度计算\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # 提取嵌入\n    batch_embeddings = outputs.last_hidden_state\n    embeddings.append(batch_embeddings.cpu())  # 移到 CPU 释放 GPU 内存\n    \n    # 清理内存\n    del inputs, outputs, batch_embeddings\n    torch.cuda.empty_cache()\n\n# 合并所有嵌入\nembeddings = torch.cat(embeddings, dim=0)\nprint(f\"Embeddings shape: {embeddings.shape}\")\n\n# 保存嵌入到磁盘\ntorch.save(embeddings, \"protein_davis.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T06:40:40.958113Z","iopub.execute_input":"2025-03-15T06:40:40.958313Z","iopub.status.idle":"2025-03-15T06:47:02.472257Z","shell.execute_reply.started":"2025-03-15T06:40:40.958293Z","shell.execute_reply":"2025-03-15T06:47:02.471481Z"}},"outputs":[{"name":"stdout","text":"Number of unique proteins: 379\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6958ae5cc1b40a8ae105327f93ff285"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/238k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e58a7b2ef0eb4c129d98e634f03f9daa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca47c2d5f24e468083b96bdb418437e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"439e0b915b0844609d4f4468065f1eee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/11.3G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a84e7a88e14870b7a334e4792d488c"}},"metadata":{}},{"name":"stdout","text":"Using 2 GPUs!\nEmbeddings shape: torch.Size([379, 1000, 1024])\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport os\nimport re\nfrom transformers import T5Tokenizer, T5EncoderModel\nfrom torch.nn.parallel import DataParallel\n\n# 设置环境变量以减少内存碎片\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# 读取和处理数据\nwith open('/kaggle/input/davis-n-kiba/KIBA.txt', 'r') as f:\n    lines = f.readlines()\n\ndata = []\nfor line in lines:\n    parts = line.strip().split(' ', 4)\n    if len(parts) == 5:\n        compound_id, protein_name, smiles, rest = parts[0], parts[1], parts[2], parts[3] + ' ' + parts[4]\n        sequence, label = rest.rsplit(' ', 1)\n        data.append({\n            'compound_id': compound_id,\n            'protein_name': protein_name,\n            'smiles': smiles,\n            'sequence': sequence,\n            'label': int(label)\n        })\n\n# 提取唯一蛋白序列\nproteins = set()\nfor protein in data:\n    proteins.add(protein['sequence'])\nproteins = list(proteins)\nprint(f\"Number of unique proteins: {len(proteins)}\")\n\n# 预处理序列\nsequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in proteins]\n\n# 设置设备\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 加载 tokenizer 和模型\nfrom transformers import T5Tokenizer, T5EncoderModel\n\n# local_path_model = \"autodl-tmp/prot_t5_xl_uniref50\"\nlocal_path_model = \"Rostlab/prot_t5_xl_uniref50\"\ntokenizer = T5Tokenizer.from_pretrained(local_path_model, legacy=False)  # 设置 legacy=False 使用新行为\n\n# 使用半精度加载模型\nmodel = T5EncoderModel.from_pretrained(local_path_model, torch_dtype=torch.float16)\n#model = T5EncoderModel.from_pretrained(local_path_model)\n\n# 如果有多个 GPU，使用 DataParallel\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = DataParallel(model)\n\nmodel = model.to(device)\nmodel.eval()\n\n# 分批处理序列\nbatch_size = 32\nembeddings = []\nmax_length = 1000\n\nfor i in range(0, len(sequence_examples), batch_size):\n    batch_sequences = sequence_examples[i:i + batch_size]\n    \n    # 明确指定 padding='max_length' 以确保所有序列填充到 max_length\n    inputs = tokenizer(batch_sequences, return_tensors=\"pt\", padding=\"max_length\", \n                       truncation=True, max_length=max_length).to(device)\n    \n    # 前向传播，无梯度计算\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # 提取嵌入\n    batch_embeddings = outputs.last_hidden_state\n    embeddings.append(batch_embeddings.cpu())  # 移到 CPU 释放 GPU 内存\n    \n    # 清理内存\n    del inputs, outputs, batch_embeddings\n    torch.cuda.empty_cache()\n\n# 合并所有嵌入\nembeddings = torch.cat(embeddings, dim=0)\nprint(f\"Embeddings shape: {embeddings.shape}\")\n\n# 保存嵌入到磁盘\ntorch.save(embeddings, \"protein_kiba.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T06:50:22.862699Z","iopub.execute_input":"2025-03-15T06:50:22.863033Z","iopub.status.idle":"2025-03-15T06:51:48.539178Z","shell.execute_reply.started":"2025-03-15T06:50:22.863004Z","shell.execute_reply":"2025-03-15T06:51:48.538190Z"}},"outputs":[{"name":"stdout","text":"Number of unique proteins: 225\nUsing 2 GPUs!\nEmbeddings shape: torch.Size([225, 1000, 1024])\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}